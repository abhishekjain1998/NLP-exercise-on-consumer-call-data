{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_complaint_length = 8\n",
    "numTopics = 200\n",
    "summary_length = 2\n",
    "similarity_threshold = 0.8\n",
    "keywords_per_label = 100\n",
    "keywords_n_grams = 2\n",
    "remove_top_n = 50\n",
    "at_least_match = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "from numpy import zeros, double\n",
    "\n",
    "import nltk\n",
    "from nltk import sent_tokenize,word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "\n",
    "import ast\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora,models\n",
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from pyemd import emd\n",
    "\n",
    "  \n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions = { \n",
    "\"ain't\": \"am not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"needn't\": \"need not\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there had\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who's\": \"who is\",\n",
    "\"won't\": \"will not\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you're\": \"you are\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, remove_stopwords = True):    \n",
    "    \n",
    "    text = text.lower()\n",
    "    \n",
    "    \n",
    "    if True:\n",
    "        text = text.split()\n",
    "        new_text = []\n",
    "        for word in text:\n",
    "            if word in contractions:\n",
    "                new_text.append(contractions[word])\n",
    "            else:\n",
    "                new_text.append(word)\n",
    "        text = \" \".join(new_text)\n",
    "    \n",
    "    \n",
    "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\<a href', ' ', text)\n",
    "    text = re.sub(r'&amp;', '', text) \n",
    "    text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', text)\n",
    "    text = re.sub(r'<br />', ' ', text)\n",
    "    text = re.sub(r'\\'', ' ', text)\n",
    "    text = re.sub(r'xxxx', ' ', text)\n",
    "    text = re.sub(r'xx', ' ', text)\n",
    "    text = re.sub(r'xxx', ' ', text)\n",
    "    \n",
    "   \n",
    "    if remove_stopwords:\n",
    "        text = text.split()\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        text = [w for w in text if not w in stops]\n",
    "        text = \" \".join(text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('complaints.csv')\n",
    "df = df[['Issue','Consumer complaint narrative']]\n",
    "df = df[df['Consumer complaint narrative'].notna()]\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Issue</th>\n",
       "      <th>Consumer complaint narrative</th>\n",
       "      <th>tokenized_sent</th>\n",
       "      <th>complaint_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Closing on a mortgage</td>\n",
       "      <td>We have already tried to contact the company w...</td>\n",
       "      <td>[We have already tried to contact the company ...</td>\n",
       "      <td>177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Incorrect information on your report</td>\n",
       "      <td>Accounts added and/or created unbeknownst to m...</td>\n",
       "      <td>[Accounts added and/or created unbeknownst to ...</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Trouble during payment process</td>\n",
       "      <td>I have been trying to get my Private Mortgage ...</td>\n",
       "      <td>[I have been trying to get my Private Mortgage...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Improper use of your report</td>\n",
       "      <td>2ND NOTICE OF PENDING LITIGATION SEEKING RELIE...</td>\n",
       "      <td>[2ND NOTICE OF PENDING LITIGATION SEEKING RELI...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Trouble during payment process</td>\n",
       "      <td>The mortgage company ( Roundpoint Mortgageg ) ...</td>\n",
       "      <td>[The mortgage company ( Roundpoint Mortgageg )...</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Issue  \\\n",
       "0                 Closing on a mortgage   \n",
       "1  Incorrect information on your report   \n",
       "2        Trouble during payment process   \n",
       "3           Improper use of your report   \n",
       "4        Trouble during payment process   \n",
       "\n",
       "                        Consumer complaint narrative  \\\n",
       "0  We have already tried to contact the company w...   \n",
       "1  Accounts added and/or created unbeknownst to m...   \n",
       "2  I have been trying to get my Private Mortgage ...   \n",
       "3  2ND NOTICE OF PENDING LITIGATION SEEKING RELIE...   \n",
       "4  The mortgage company ( Roundpoint Mortgageg ) ...   \n",
       "\n",
       "                                      tokenized_sent  complaint_length  \n",
       "0  [We have already tried to contact the company ...               177  \n",
       "1  [Accounts added and/or created unbeknownst to ...                30  \n",
       "2  [I have been trying to get my Private Mortgage...                14  \n",
       "3  [2ND NOTICE OF PENDING LITIGATION SEEKING RELI...                10  \n",
       "4  [The mortgage company ( Roundpoint Mortgageg )...                20  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tokenized_sent'] = df['Consumer complaint narrative'].apply(lambda x: sent_tokenize(x))\n",
    "df['complaint_length'] = df['tokenized_sent'].apply(lambda x: len(x))\n",
    "df = df[df['complaint_length']>=min_complaint_length]\n",
    "df = df.reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Issue</th>\n",
       "      <th>Consumer complaint narrative</th>\n",
       "      <th>tokenized_sent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Closing on a mortgage</td>\n",
       "      <td>We have already tried to contact the company w...</td>\n",
       "      <td>[We have already tried to contact the company ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Incorrect information on your report</td>\n",
       "      <td>Accounts added and/or created unbeknownst to m...</td>\n",
       "      <td>[Accounts added and/or created unbeknownst to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Trouble during payment process</td>\n",
       "      <td>I have been trying to get my Private Mortgage ...</td>\n",
       "      <td>[I have been trying to get my Private Mortgage...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Improper use of your report</td>\n",
       "      <td>2ND NOTICE OF PENDING LITIGATION SEEKING RELIE...</td>\n",
       "      <td>[2ND NOTICE OF PENDING LITIGATION SEEKING RELI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Trouble during payment process</td>\n",
       "      <td>The mortgage company ( Roundpoint Mortgageg ) ...</td>\n",
       "      <td>[The mortgage company ( Roundpoint Mortgageg )...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Issue  \\\n",
       "0                 Closing on a mortgage   \n",
       "1  Incorrect information on your report   \n",
       "2        Trouble during payment process   \n",
       "3           Improper use of your report   \n",
       "4        Trouble during payment process   \n",
       "\n",
       "                        Consumer complaint narrative  \\\n",
       "0  We have already tried to contact the company w...   \n",
       "1  Accounts added and/or created unbeknownst to m...   \n",
       "2  I have been trying to get my Private Mortgage ...   \n",
       "3  2ND NOTICE OF PENDING LITIGATION SEEKING RELIE...   \n",
       "4  The mortgage company ( Roundpoint Mortgageg ) ...   \n",
       "\n",
       "                                      tokenized_sent  \n",
       "0  [We have already tried to contact the company ...  \n",
       "1  [Accounts added and/or created unbeknownst to ...  \n",
       "2  [I have been trying to get my Private Mortgage...  \n",
       "3  [2ND NOTICE OF PENDING LITIGATION SEEKING RELI...  \n",
       "4  [The mortgage company ( Roundpoint Mortgageg )...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = df.head(5000)\n",
    "df1 = df1.drop(['complaint_length'],axis=1)\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_scam = ['Improper use of your report','Fraud or scam','Problem with fraud alerts or security freezes','Threatened to contact someone or share information improperly','Taking/threatening an illegal action']\n",
    "company_related = ['Took or threatened to take negative or legal action','Other features, terms, or problems','Confusing or misleading advertising or marketing','Problem with additional add-on products or services','Advertising','Identity theft protection or other monitoring services','Other service problem','Problem with customer service','Confusing or missing disclosures',\"Problem with a company's investigation into an existing issue\",'Advertising and marketing, including promotional offers','Communication tactics','Dealing with your lender or servicer','Problem with a lender or other company charging your account',\"Can't contact lender or servicer\"]\n",
    "account = ['Closing your account',\"Can't stop withdrawals from your bank account\",'Problem with overdraft','Lost or stolen check','Problem adding money','Money was taken from your bank account on the wrong day or for the wrong amount','Lost or stolen money order','Closing an account','Opening an account','Managing an account','Incorrect information on your report','False statements or representation','Managing, opening, or closing your mobile wallet account']\n",
    "transactions = ['Problem caused by your funds being low','Problem with a purchase shown on your statement','Trouble during payment process','Unexpected or other fees','Other transaction problem','Fees or interest','Problem when making payments',\"Charged fees or interest you didn't expect\",'Struggling to pay your bill','Unauthorized transactions or other transaction problem','Wrong amount charged or received','Problems when you are unable to pay',\"Charged fees or interest I didn't expect\",'Settlement process and costs','Problem with a purchase or transfer','Money was not available when promised']\n",
    "debt = ['Written notification about debt','Attempts to collect debt not owed',\"Cont'd attempts collect debt not owed\",'Disclosure verification of debt']\n",
    "loan = ['Vehicle was repossessed or sold the vehicle','Struggling to pay your loan','Problems at the end of the loan or lease','Managing the loan or lease','Shopping for a loan or lease', 'Struggling to repay your loan','Getting a loan or lease','Getting the loan',\"Was approved for a loan, but didn't receive money\",'Problem with the payoff process at the end of the loan',\"Received a loan I didn't apply for\",'Loan servicing, payments, escrow account','Taking out the loan or lease',\"Loan payment wasn't credited to your account\",'Getting a loan',\"Received a loan you didn't apply for\",\"Was approved for a loan, but didn't receive the money\"]\n",
    "credit_service_related = [\"Problem with a credit reporting company's investigation into an existing problem\",'Credit monitoring or identity theft protection services','Unable to get your credit report or credit score','Getting a line of credit']\n",
    "card_related = ['Trouble using the card','Problem getting a card or closing an account','Getting a credit card','Trouble using your card']\n",
    "mortgage = ['Closing on a mortgage','Struggling to pay mortgage','Applying for a mortgage or refinancing an existing mortgage','Applying for a mortgage','Application, originator, mortgage broker','Payoff process']\n",
    "other = ['Other']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_list = [loan,credit_service_related,card_related,mortgage,debt,transactions,account,company_related,fraud_scam,other]\n",
    "label = ['loan','credit_service','card','mortgage','debt','transactions','account','company_issue','fraud_scam','others']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_label(x,label_list,label):\n",
    "    for i in range(len(labels_list)):\n",
    "        for j in labels_list[i]:\n",
    "            if x==j:\n",
    "                return label[i]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.insert(1, \"label\", [None]*len(df1), True)\n",
    "df1['label'] = df1['Issue'].apply(lambda x: assign_label(x,labels_list,label))\n",
    "df1 = df1.drop(['Issue'],axis=1)\n",
    "df1.rename(columns = {'label':'Issue'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df1['tokenized_sent'])):\n",
    "    comp = df1['tokenized_sent'][i]\n",
    "    for j in comp:\n",
    "        v = word_tokenize(j)\n",
    "        if len(v)==1:\n",
    "            df1['tokenized_sent'][i].remove(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ngrams(data, num):\n",
    "    n_grams = TextBlob(data).ngrams(num)\n",
    "    return [ ' '.join(grams) for grams in n_grams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "f =  open('keywords_file.txt')\n",
    "keyword = f.readlines()    \n",
    "f.close()\n",
    "\n",
    "keywords_list = [None]*len(keyword)\n",
    "for i in range(len(keyword)):\n",
    "    keywords_list[i] = ast.literal_eval(keyword[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove = []\n",
    "for i in range(len(df1)):\n",
    "    comp = df1['Consumer complaint narrative'][i]\n",
    "    issue_label = df1['Issue'][i]    \n",
    "    comp = clean_text(comp)\n",
    "    comp = lemmatizer.lemmatize(comp)\n",
    "    comp2 = extract_ngrams(comp,2)\n",
    "    comp1 = extract_ngrams(comp,1)\n",
    "    comp = comp1+comp2\n",
    "    \n",
    "    for j in range(len(label)):\n",
    "        if issue_label==label[j]:\n",
    "            key = keywords_list[j]\n",
    "            match = list(set(comp).intersection(key))\n",
    "            \n",
    "            if len(match) == 0:\n",
    "                remove.append(i)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes_to_keep = set(range(df1.shape[0])) - set(remove)\n",
    "df1 = df1.take(list(indexes_to_keep))\n",
    "df1.reset_index(inplace = True, drop = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.insert(3, \"words_of_sents\", [None]*len(df1), True)\n",
    "df1.insert(4, \"vectors_of_corpus\", [None]*len(df1), True)\n",
    "df1.insert(5, \"vectors_of_sents\", [None]*len(df1), True)\n",
    "df1.insert(6, \"summary\", [None]*len(df1), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = ''\n",
    "for i in range(len(df1)):\n",
    "    v = df1['tokenized_sent'][i]\n",
    "    for j in v:\n",
    "        corpus+=j\n",
    "\n",
    "w2v = models.Word2Vec(sentences=corpus, min_count=5,size=200)\n",
    "w2v.save(\"word2vec.model\")\n",
    "w2v =  models.Word2Vec.load(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wmdistance(self, document1, document2):\n",
    "    len_pre_oov1 = len(document1)\n",
    "    len_pre_oov2 = len(document2)\n",
    "    document1 = [token for token in document1 if token in self]\n",
    "    document2 = [token for token in document2 if token in self]\n",
    "    diff1 = len_pre_oov1 - len(document1)\n",
    "    diff2 = len_pre_oov2 - len(document2)\n",
    "    \n",
    "    dictionary = Dictionary(documents=[document1, document2])\n",
    "    vocab_len = len(dictionary)\n",
    "\n",
    "    # Sets for faster look-up.\n",
    "    docset1 = set(document1)\n",
    "    docset2 = set(document2)\n",
    "\n",
    "    # Compute distance matrix.\n",
    "    distance_matrix = zeros((vocab_len, vocab_len), dtype=double)\n",
    "    for i, t1 in dictionary.items():\n",
    "        for j, t2 in dictionary.items():\n",
    "            if not t1 in docset1 or not t2 in docset2:\n",
    "                continue\n",
    "            # Compute cosine distance between word vectors.\n",
    "            distance_matrix[i, j] = dot(self[t1],self[t2])/(norm(self[t1])*norm(self[t2]))           \n",
    "            \n",
    "    def nbow(document):\n",
    "        d = zeros(vocab_len, dtype=double)\n",
    "        nbow = dictionary.doc2bow(document)  # Word frequencies.\n",
    "        doc_len = len(document)\n",
    "        for idx, freq in nbow:\n",
    "            d[idx] = freq / float(doc_len)  # Normalized word frequencies.\n",
    "        return d\n",
    "    \n",
    "    d1 = nbow(document1)\n",
    "    d2 = nbow(document2)\n",
    "    return emd(d1, d2, distance_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I have been trying to get my Private Mortgage Insurance Removed from my mortgage since XX/XX/XXXX when my mortgage dropped below 80 % loan to value.',\n",
       " 'Last year my mortgage was sold from XXXX XXXX  ( Under mortgage # XXXX ) to Ditech Mortgage ( account # XXXX ).',\n",
       " 'I reached out to Ditech via a email ( after being told to do so via phone representative ) request to remove my PMI on mortgage on XX/XX/XXXX and received no response at all from them, I even checked my junk box and nothing was there.',\n",
       " 'My mortgage papers that I signed state an \" Automatic Termination of PMI \\'\\' that states once my loan is below 78 % loan to value PMI will automatically terminate ( I have attached this document ).',\n",
       " 'I reached out again today on XX/XX/XXXX to make this request via phone and was told initially to send the request that I already sent it too.',\n",
       " 'I asked to speak with a supervisor and after being put on hold for about 30 minutes, I finally spoke to one.',\n",
       " 'They told me that my loan to value must be under 70 % loan to value and that was their policy.',\n",
       " 'After reading this document to the supervisor, I was told that \" they don\\'t have that document on file \\'\\'.',\n",
       " 'She ( XXXX XXXX ) sent me a link to send her the form I have.',\n",
       " \"I did so and just told me that I'll be hearing from them in 7-10 business days.\",\n",
       " 'Given their past history, I highly doubt that I will hear from them.',\n",
       " \"I did mention to the supervisor and ask her why they weren't staying compliant to the homeowners protection act and she said nothing.\",\n",
       " 'From my understand this act requires mortgage companies to drop off PMI once loans are below 78 % LTV and the loan is current.',\n",
       " \"I qualify for both of those items and don't understand why this is such a difficult task.\"]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['tokenized_sent'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = 'I have been trying to get my Private Mortgage Insurance Removed from my mortgage since XX/XX/XXXX when my mortgage dropped below 80 % loan to value.'\n",
    "doc2 = 'From my understand this act requires mortgage companies to drop off PMI once loans are below 78 % LTV and the loan is current.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist1 = wmdistance(w2v.wv,doc1,doc2)\n",
    "dist2 = w2v.wv.wmdistance(doc1, doc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.020655346431308765"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0062427529863539905"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(sentences_list):\n",
    "    \n",
    "    words_list = [None]*len(sentences_list)\n",
    "    for i in range(len(sentences_list)):\n",
    "        words_list[i] = word_tokenize(sentences_list[i])\n",
    "    \n",
    "    return words_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def takeFirst(x):\n",
    "    return x[0]\n",
    "\n",
    "def takeSecond(x):\n",
    "    return x[1]\n",
    "\n",
    "def selTopSents(summSize, numTopics, sortedVecs):\n",
    "    topSentences = []\n",
    "    sentIndexes = set()\n",
    "    sCount = 0\n",
    "    \n",
    "    for i in range(summSize):\n",
    "        for j in range(numTopics):\n",
    "            vecs = sortedVecs[j]                       \n",
    "            si = vecs[i][0]\n",
    "            \n",
    "            if si not in sentIndexes:\n",
    "                topSentences.append(vecs[i])\n",
    "                sentIndexes.add(si)\n",
    "                sCount += 1\n",
    "            if sCount == summSize:\n",
    "                return topSentences\n",
    "\n",
    "def lsi(sentTokens,numTopics):\n",
    "    \n",
    "    dct = corpora.Dictionary(sentTokens)\n",
    "    corpus = list(map(lambda st: dct.doc2bow(st), sentTokens))    \n",
    "    lsi = models.LsiModel(corpus, id2word=dct,num_topics=numTopics)\n",
    "    \n",
    "    vecCorpus = lsi[corpus]\n",
    "   \n",
    "    return vecCorpus            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Cumulative(lists):  \n",
    "    cu_list = []  \n",
    "    length = len(lists)  \n",
    "    cu_list = [sum(lists[0:x:1]) for x in range(0, length+1)]  \n",
    "    return cu_list[1:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectors_of_sentences(x,numTopics):\n",
    "    vectors = [None]*len(x)\n",
    "    for i,dv in enumerate(x):\n",
    "        array = [None]*numTopics\n",
    "        for sc in dv:\n",
    "            array[sc[0]] = sc[1]\n",
    "        vectors[i] = array\n",
    "        \n",
    "    return vectors  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_complaint_vectors(tokenized_sent,numTopics):\n",
    "    \n",
    "    df1['words_of_sents'] = tokenized_sent.apply(lambda y: tokenization(y))\n",
    "    complaint_corpus = []\n",
    "    \n",
    "    for i in range(len(df1['words_of_sents'])):\n",
    "        complaint_corpus += df1['words_of_sents'][i]\n",
    "\n",
    "    vectors_of_complaints = lsi(complaint_corpus,numTopics)\n",
    "\n",
    "    lengths_of_sentences = []\n",
    "    \n",
    "    for i in range(len(df1['words_of_sents'])):\n",
    "        lengths_of_sentences.append(len(df1['words_of_sents'][i]))\n",
    "    lengths_of_sentences.insert(0,0)\n",
    "\n",
    "    l = Cumulative(lengths_of_sentences)\n",
    "\n",
    "\n",
    "    for i in range(len(lengths_of_sentences)-1):\n",
    "        df1['vectors_of_corpus'][i] = vectors_of_complaints[l[i]:l[i+1]]\n",
    "            \n",
    "    df1['vectors_of_sents'] = df1['vectors_of_corpus'].apply(lambda x: vectors_of_sentences(x,numTopics))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lsi_summ(vecCorpus,numTopics,summary_length,sents):    \n",
    "\n",
    "    b=[]\n",
    "    \n",
    "    for i in range(len(df1['tokenized_sent'])):\n",
    "        if len(df1['tokenized_sent'][i])==1:\n",
    "            b.append(i)\n",
    "    \n",
    "    l = list(range(len(df1)))\n",
    "    \n",
    "    r = list(set(l)^set(b))\n",
    "    \n",
    "    for i in r:\n",
    "        \n",
    "        sortedVecs = list(map(lambda x: list(), range(numTopics)))\n",
    "\n",
    "        for j,dv in enumerate(vecCorpus[i]):\n",
    "            for sc in dv:\n",
    "                isc = (j, abs(sc[1]))\n",
    "                sortedVecs[sc[0]].append(isc)\n",
    "\n",
    "        sortedVecs = list(map(lambda iscl: sorted(iscl,key=takeSecond,reverse=True), sortedVecs))\n",
    "          \n",
    "    \n",
    "        top_sents = selTopSents(summary_length,numTopics,sortedVecs)       \n",
    "        top_sents = sorted(top_sents,key=takeFirst)\n",
    "        top_sentences = list(map(lambda ts: (ts[0],sents[i][ts[0]], ts[1]), top_sents))\n",
    "        \n",
    "        df1['summary'][i] = top_sentences\n",
    "        \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "summ_df = pd.DataFrame()\n",
    "summ_df.insert(0, \"summary\", [None]*len(df1), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summ_modification_index():\n",
    "    \n",
    "    x = []\n",
    "    modified = 0\n",
    "    for i in range(len(df1)):\n",
    "        temp1 = summ_df['summary'][i]\n",
    "        temp2 = df1['summary'][i]\n",
    "        x1 = []\n",
    "        x2 = []\n",
    "        for j in range(summary_length):\n",
    "            x1.append(temp1[j][1])\n",
    "            x2.append(temp2[j][1])\n",
    "        \n",
    "        x.append(list(set(x1)^set(x2)))\n",
    "        \n",
    "    for i in range(len(df1)):\n",
    "        if x[i]!=[]:\n",
    "            modified+=1\n",
    "    return round((modified/len(df1)),4)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class garbage_dictionary(dict): \n",
    "  \n",
    "    \n",
    "    def __init__(self): \n",
    "        self = dict() \n",
    "          \n",
    "    \n",
    "    def add(self, key, value): \n",
    "        self[key] = value \n",
    "        \n",
    "blacklist = garbage_dictionary()\n",
    "for i in range(len(df1)):\n",
    "        blacklist.add(i,[None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_blacklist(bl):\n",
    "    \n",
    "    f =  open('blacklist_w2v.txt')\n",
    "    black = f.readlines()    \n",
    "    f.close()\n",
    "    \n",
    "    i = input(\"pruning round # : \")\n",
    "    b = black[int(i)]\n",
    "    b = b.split(' ')\n",
    "    \n",
    "    for j in b:\n",
    "        if j=='\\n':\n",
    "            b.remove(j)\n",
    "\n",
    "    for j in b:\n",
    "        j = j.split(':')\n",
    "        key = int(j[0])\n",
    "        value = ast.literal_eval(j[1])\n",
    "        bl.add(key,value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_remove(comp,lis):\n",
    "    sent_to_remove = []\n",
    "    if lis!=[]:\n",
    "        for j in lis:\n",
    "            sent_to_remove.append(comp[j])\n",
    "        for k in sent_to_remove:\n",
    "            comp.remove(k)\n",
    "            \n",
    "def vect_remove(comp,lis):\n",
    "    vect_to_remove = []\n",
    "    if lis!=[]:\n",
    "        for j in lis:\n",
    "            vect_to_remove.append(comp[j])\n",
    "        for k in vect_to_remove:\n",
    "            comp.remove(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_sents(blacklist,comp_text,numTopics,model):\n",
    "    \n",
    "    b = []\n",
    "    for k in blacklist.keys():\n",
    "        if blacklist[k]!=[None]:\n",
    "            b.append(k)\n",
    "    \n",
    "    l = list(range(len(df1)))\n",
    "    \n",
    "    r = list(set(l)^set(b))\n",
    "    \n",
    " \n",
    "    comp_sim = list(map(lambda i: list(), range(len(df1))))\n",
    "    \n",
    "    for i in r:\n",
    "        complaint_text = df1['tokenized_sent'][i]        \n",
    "        for j in range(len(complaint_text)):\n",
    "            text = complaint_text[j]\n",
    "#             eucliden distance in wmd\n",
    "#             sim = model.wv.wmdistance(text, comp_text)\n",
    "#             cosine distance in wmd\n",
    "            sim = wmdistance(model.wv,text,comp_text)\n",
    "            comp_sim[i].append([i,sim,j])         \n",
    "            \n",
    "    x = []\n",
    "    for i in r:\n",
    "        x += comp_sim[i]\n",
    "    \n",
    "    top_sim = sorted(x,key=takeSecond,reverse=False)\n",
    "    x = None\n",
    "    top_sim = top_sim[0:50]\n",
    "    \n",
    "    cos_sim = list(map(lambda i: list(), range(len(df1))))\n",
    "    \n",
    "    for i in range(len(top_sim)):\n",
    "        index = top_sim[i][0]\n",
    "        sentence_id = top_sim[i][2]\n",
    "        cos_sim[index].append(sentence_id)    \n",
    "            \n",
    "    \n",
    "    for i in range(len(cos_sim)):\n",
    "        \n",
    "        sent_remove(df1['tokenized_sent'][i],cos_sim[i])\n",
    "        vect_remove(df1['vectors_of_sents'][i],cos_sim[i])       \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_data(blacklist,numTopics,model):\n",
    "    \n",
    "    for i in range(len(blacklist)):\n",
    "        \n",
    "        if blacklist[i]!=[None]:\n",
    "            \n",
    "            t = df1['tokenized_sent'][i]\n",
    "            \n",
    "            vect_remove(df1['vectors_of_sents'][i],blacklist[i])\n",
    "            \n",
    "            for j in blacklist[i]:                                             \n",
    "                ref_t = t[j]\n",
    "                remove_sents(blacklist,ref_t,numTopics,model)\n",
    "                \n",
    "            sent_remove(df1['tokenized_sent'][i],blacklist[i])          \n",
    "                \n",
    "    \n",
    "    for i in range(len(df1)):\n",
    "        summ_df['summary'][i] = df1['summary'][i]\n",
    "    \n",
    "    blacklist.clear()\n",
    "    for i in range(len(df1)):\n",
    "        blacklist.add(i,[None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "modification_index = []\n",
    "relevance = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevance_score(keywords_n_grams):\n",
    "    \n",
    "    correct_pred = 0\n",
    "    for j in range(len(df1)):\n",
    "            x = df1['summary'][j]\n",
    "            issue = df1['Issue'][j]\n",
    "\n",
    "            match = []\n",
    "            key = None\n",
    "            summ =' '\n",
    "            for i in range(len(x)):\n",
    "                summ+= x[i][1]\n",
    "            summ = clean_text(summ)\n",
    "            summ = lemmatizer.lemmatize(summ)\n",
    "            summ1 = extract_ngrams(summ,1)\n",
    "            summ2 = extract_ngrams(summ,2)\n",
    "            summ = summ1+summ2\n",
    "            \n",
    "            for k in range(len(label)):\n",
    "                if label[k]==issue:\n",
    "                    key = keywords_list[k]\n",
    "                    \n",
    "            match = list(set(summ).intersection(key))\n",
    "            \n",
    "            if len(match)>1:\n",
    "                correct_pred+=1              \n",
    "\n",
    "\n",
    "    return round((correct_pred/len(df1)),4)   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_complaint_vectors(df1['tokenized_sent'],numTopics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsi_summ(df1['vectors_of_corpus'],numTopics,summary_length,df1['tokenized_sent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "modification_index.append(summ_modification_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevance.append(get_relevance_score(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['Issue'][]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['summary'][]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_blacklist = open(\"blacklist_w2v.txt\",mode=\"a\",encoding=\"utf-8\")\n",
    "add_blacklist.write('\\n')\n",
    "add_blacklist.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pruning round # : 0\n"
     ]
    }
   ],
   "source": [
    "update_blacklist(blacklist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prune_data(blacklist,numTopics,w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
