{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_complaint_length = 8\n",
    "numTopics = 200\n",
    "summary_length = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "import nltk\n",
    "from nltk import sent_tokenize,word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "\n",
    "import ast\n",
    "\n",
    "from gensim import corpora,models\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "  \n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions = { \n",
    "\"ain't\": \"am not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"needn't\": \"need not\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there had\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who's\": \"who is\",\n",
    "\"won't\": \"will not\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you're\": \"you are\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, remove_stopwords = True):    \n",
    "    \n",
    "    text = text.lower()\n",
    "    \n",
    "    \n",
    "    if True:\n",
    "        text = text.split()\n",
    "        new_text = []\n",
    "        for word in text:\n",
    "            if word in contractions:\n",
    "                new_text.append(contractions[word])\n",
    "            else:\n",
    "                new_text.append(word)\n",
    "        text = \" \".join(new_text)\n",
    "    \n",
    "    \n",
    "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\<a href', ' ', text)\n",
    "    text = re.sub(r'&amp;', '', text) \n",
    "    text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', text)\n",
    "    text = re.sub(r'<br />', ' ', text)\n",
    "    text = re.sub(r'\\'', ' ', text)\n",
    "    text = re.sub(r'xxxx', ' ', text)\n",
    "    text = re.sub(r'xx', ' ', text)\n",
    "    text = re.sub(r'xxx', ' ', text)\n",
    "    \n",
    "   \n",
    "    if remove_stopwords:\n",
    "        text = text.split()\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        text = [w for w in text if not w in stops]\n",
    "        text = \" \".join(text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('complaints.csv')\n",
    "df = df[['Issue','Consumer complaint narrative']]\n",
    "df = df[df['Consumer complaint narrative'].notna()]\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Issue</th>\n",
       "      <th>Consumer complaint narrative</th>\n",
       "      <th>tokenized_sent</th>\n",
       "      <th>complaint_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Closing on a mortgage</td>\n",
       "      <td>We have already tried to contact the company w...</td>\n",
       "      <td>[We have already tried to contact the company ...</td>\n",
       "      <td>177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Incorrect information on your report</td>\n",
       "      <td>Accounts added and/or created unbeknownst to m...</td>\n",
       "      <td>[Accounts added and/or created unbeknownst to ...</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Trouble during payment process</td>\n",
       "      <td>I have been trying to get my Private Mortgage ...</td>\n",
       "      <td>[I have been trying to get my Private Mortgage...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Improper use of your report</td>\n",
       "      <td>2ND NOTICE OF PENDING LITIGATION SEEKING RELIE...</td>\n",
       "      <td>[2ND NOTICE OF PENDING LITIGATION SEEKING RELI...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Trouble during payment process</td>\n",
       "      <td>The mortgage company ( Roundpoint Mortgageg ) ...</td>\n",
       "      <td>[The mortgage company ( Roundpoint Mortgageg )...</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Issue  \\\n",
       "0                 Closing on a mortgage   \n",
       "1  Incorrect information on your report   \n",
       "2        Trouble during payment process   \n",
       "3           Improper use of your report   \n",
       "4        Trouble during payment process   \n",
       "\n",
       "                        Consumer complaint narrative  \\\n",
       "0  We have already tried to contact the company w...   \n",
       "1  Accounts added and/or created unbeknownst to m...   \n",
       "2  I have been trying to get my Private Mortgage ...   \n",
       "3  2ND NOTICE OF PENDING LITIGATION SEEKING RELIE...   \n",
       "4  The mortgage company ( Roundpoint Mortgageg ) ...   \n",
       "\n",
       "                                      tokenized_sent  complaint_length  \n",
       "0  [We have already tried to contact the company ...               177  \n",
       "1  [Accounts added and/or created unbeknownst to ...                30  \n",
       "2  [I have been trying to get my Private Mortgage...                14  \n",
       "3  [2ND NOTICE OF PENDING LITIGATION SEEKING RELI...                10  \n",
       "4  [The mortgage company ( Roundpoint Mortgageg )...                20  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tokenized_sent'] = df['Consumer complaint narrative'].apply(lambda x: sent_tokenize(x))\n",
    "df['complaint_length'] = df['tokenized_sent'].apply(lambda x: len(x))\n",
    "df = df[df['complaint_length']>=min_complaint_length]\n",
    "df = df.reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Issue</th>\n",
       "      <th>Consumer complaint narrative</th>\n",
       "      <th>tokenized_sent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Closing on a mortgage</td>\n",
       "      <td>We have already tried to contact the company w...</td>\n",
       "      <td>[We have already tried to contact the company ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Incorrect information on your report</td>\n",
       "      <td>Accounts added and/or created unbeknownst to m...</td>\n",
       "      <td>[Accounts added and/or created unbeknownst to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Trouble during payment process</td>\n",
       "      <td>I have been trying to get my Private Mortgage ...</td>\n",
       "      <td>[I have been trying to get my Private Mortgage...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Improper use of your report</td>\n",
       "      <td>2ND NOTICE OF PENDING LITIGATION SEEKING RELIE...</td>\n",
       "      <td>[2ND NOTICE OF PENDING LITIGATION SEEKING RELI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Trouble during payment process</td>\n",
       "      <td>The mortgage company ( Roundpoint Mortgageg ) ...</td>\n",
       "      <td>[The mortgage company ( Roundpoint Mortgageg )...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Issue  \\\n",
       "0                 Closing on a mortgage   \n",
       "1  Incorrect information on your report   \n",
       "2        Trouble during payment process   \n",
       "3           Improper use of your report   \n",
       "4        Trouble during payment process   \n",
       "\n",
       "                        Consumer complaint narrative  \\\n",
       "0  We have already tried to contact the company w...   \n",
       "1  Accounts added and/or created unbeknownst to m...   \n",
       "2  I have been trying to get my Private Mortgage ...   \n",
       "3  2ND NOTICE OF PENDING LITIGATION SEEKING RELIE...   \n",
       "4  The mortgage company ( Roundpoint Mortgageg ) ...   \n",
       "\n",
       "                                      tokenized_sent  \n",
       "0  [We have already tried to contact the company ...  \n",
       "1  [Accounts added and/or created unbeknownst to ...  \n",
       "2  [I have been trying to get my Private Mortgage...  \n",
       "3  [2ND NOTICE OF PENDING LITIGATION SEEKING RELI...  \n",
       "4  [The mortgage company ( Roundpoint Mortgageg )...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = df.head(5000)\n",
    "df1 = df1.drop(['complaint_length'],axis=1)\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_scam = ['Improper use of your report','Fraud or scam','Problem with fraud alerts or security freezes','Threatened to contact someone or share information improperly','Taking/threatening an illegal action']\n",
    "company_related = ['Took or threatened to take negative or legal action','Other features, terms, or problems','Confusing or misleading advertising or marketing','Problem with additional add-on products or services','Advertising','Identity theft protection or other monitoring services','Other service problem','Problem with customer service','Confusing or missing disclosures',\"Problem with a company's investigation into an existing issue\",'Advertising and marketing, including promotional offers','Communication tactics','Dealing with your lender or servicer','Problem with a lender or other company charging your account',\"Can't contact lender or servicer\"]\n",
    "account = ['Closing your account',\"Can't stop withdrawals from your bank account\",'Problem with overdraft','Lost or stolen check','Problem adding money','Money was taken from your bank account on the wrong day or for the wrong amount','Lost or stolen money order','Closing an account','Opening an account','Managing an account','Incorrect information on your report','False statements or representation','Managing, opening, or closing your mobile wallet account']\n",
    "transactions = ['Problem caused by your funds being low','Problem with a purchase shown on your statement','Trouble during payment process','Unexpected or other fees','Other transaction problem','Fees or interest','Problem when making payments',\"Charged fees or interest you didn't expect\",'Struggling to pay your bill','Unauthorized transactions or other transaction problem','Wrong amount charged or received','Problems when you are unable to pay',\"Charged fees or interest I didn't expect\",'Settlement process and costs','Problem with a purchase or transfer','Money was not available when promised']\n",
    "debt = ['Written notification about debt','Attempts to collect debt not owed',\"Cont'd attempts collect debt not owed\",'Disclosure verification of debt']\n",
    "loan = ['Vehicle was repossessed or sold the vehicle','Struggling to pay your loan','Problems at the end of the loan or lease','Managing the loan or lease','Shopping for a loan or lease', 'Struggling to repay your loan','Getting a loan or lease','Getting the loan',\"Was approved for a loan, but didn't receive money\",'Problem with the payoff process at the end of the loan',\"Received a loan I didn't apply for\",'Loan servicing, payments, escrow account','Taking out the loan or lease',\"Loan payment wasn't credited to your account\",'Getting a loan',\"Received a loan you didn't apply for\",\"Was approved for a loan, but didn't receive the money\"]\n",
    "credit_service_related = [\"Problem with a credit reporting company's investigation into an existing problem\",'Credit monitoring or identity theft protection services','Unable to get your credit report or credit score','Getting a line of credit']\n",
    "card_related = ['Trouble using the card','Problem getting a card or closing an account','Getting a credit card','Trouble using your card']\n",
    "mortgage = ['Closing on a mortgage','Struggling to pay mortgage','Applying for a mortgage or refinancing an existing mortgage','Applying for a mortgage','Application, originator, mortgage broker','Payoff process']\n",
    "other = ['Other']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_list = [loan,credit_service_related,card_related,mortgage,debt,transactions,account,company_related,fraud_scam,other]\n",
    "label = ['loan','credit_service','card','mortgage','debt','transactions','account','company_issue','fraud_scam','others']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_label(x,label_list,label):\n",
    "    for i in range(len(labels_list)):\n",
    "        for j in labels_list[i]:\n",
    "            if x==j:\n",
    "                return label[i]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.insert(1, \"label\", [None]*len(df1), True)\n",
    "df1['label'] = df1['Issue'].apply(lambda x: assign_label(x,labels_list,label))\n",
    "df1 = df1.drop(['Issue'],axis=1)\n",
    "df1.rename(columns = {'label':'Issue'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Issue</th>\n",
       "      <th>Consumer complaint narrative</th>\n",
       "      <th>tokenized_sent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mortgage</td>\n",
       "      <td>We have already tried to contact the company w...</td>\n",
       "      <td>[We have already tried to contact the company ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>account</td>\n",
       "      <td>Accounts added and/or created unbeknownst to m...</td>\n",
       "      <td>[Accounts added and/or created unbeknownst to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>transactions</td>\n",
       "      <td>I have been trying to get my Private Mortgage ...</td>\n",
       "      <td>[I have been trying to get my Private Mortgage...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fraud_scam</td>\n",
       "      <td>2ND NOTICE OF PENDING LITIGATION SEEKING RELIE...</td>\n",
       "      <td>[2ND NOTICE OF PENDING LITIGATION SEEKING RELI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>transactions</td>\n",
       "      <td>The mortgage company ( Roundpoint Mortgageg ) ...</td>\n",
       "      <td>[The mortgage company ( Roundpoint Mortgageg )...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Issue                       Consumer complaint narrative  \\\n",
       "0      mortgage  We have already tried to contact the company w...   \n",
       "1       account  Accounts added and/or created unbeknownst to m...   \n",
       "2  transactions  I have been trying to get my Private Mortgage ...   \n",
       "3    fraud_scam  2ND NOTICE OF PENDING LITIGATION SEEKING RELIE...   \n",
       "4  transactions  The mortgage company ( Roundpoint Mortgageg ) ...   \n",
       "\n",
       "                                      tokenized_sent  \n",
       "0  [We have already tried to contact the company ...  \n",
       "1  [Accounts added and/or created unbeknownst to ...  \n",
       "2  [I have been trying to get my Private Mortgage...  \n",
       "3  [2ND NOTICE OF PENDING LITIGATION SEEKING RELI...  \n",
       "4  [The mortgage company ( Roundpoint Mortgageg )...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df1['tokenized_sent'])):\n",
    "    comp = df1['tokenized_sent'][i]\n",
    "    for j in comp:\n",
    "        v = word_tokenize(j)\n",
    "        if len(v)==1:\n",
    "            df1['tokenized_sent'][i].remove(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ngrams(data, num):\n",
    "    n_grams = TextBlob(data).ngrams(num)\n",
    "    return [ ' '.join(grams) for grams in n_grams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "f =  open('keywords_file.txt')\n",
    "keyword = f.readlines()    \n",
    "f.close()\n",
    "\n",
    "keywords_list = [None]*len(keyword)\n",
    "for i in range(len(keyword)):\n",
    "    keywords_list[i] = ast.literal_eval(keyword[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove = []\n",
    "for i in range(len(df1)):\n",
    "    comp = df1['Consumer complaint narrative'][i]\n",
    "    issue_label = df1['Issue'][i]    \n",
    "    comp = clean_text(comp)\n",
    "    comp = lemmatizer.lemmatize(comp)\n",
    "    comp2 = extract_ngrams(comp,2)\n",
    "    comp1 = extract_ngrams(comp,1)\n",
    "    comp = comp1+comp2\n",
    "    \n",
    "    for j in range(len(label)):\n",
    "        if issue_label==label[j]:\n",
    "            key = keywords_list[j]\n",
    "            match = list(set(comp).intersection(key))\n",
    "            \n",
    "            if len(match) == 0:\n",
    "                remove.append(i)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes_to_keep = set(range(df1.shape[0])) - set(remove)\n",
    "df1 = df1.take(list(indexes_to_keep))\n",
    "df1.reset_index(inplace = True, drop = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.insert(3, \"words_of_sents\", [None]*len(df1), True)\n",
    "df1.insert(4, \"vectors_of_corpus\", [None]*len(df1), True)\n",
    "df1.insert(5, \"vectors_of_sents\", [None]*len(df1), True)\n",
    "df1.insert(6, \"summary\", [None]*len(df1), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(sentences_list):\n",
    "    \n",
    "    words_list = [None]*len(sentences_list)\n",
    "    for i in range(len(sentences_list)):\n",
    "        words_list[i] = word_tokenize(sentences_list[i])\n",
    "    \n",
    "    return words_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def takeFirst(x):\n",
    "    return x[0]\n",
    "\n",
    "def takeSecond(x):\n",
    "    return x[1]\n",
    "\n",
    "def selTopSents(summSize, numTopics, sortedVecs):\n",
    "    topSentences = []\n",
    "    sentIndexes = set()\n",
    "    sCount = 0\n",
    "    \n",
    "    for i in range(summSize):\n",
    "        for j in range(numTopics):\n",
    "            vecs = sortedVecs[j]                       \n",
    "            si = vecs[i][0]\n",
    "            \n",
    "            if si not in sentIndexes:\n",
    "                topSentences.append(vecs[i])\n",
    "                sentIndexes.add(si)\n",
    "                sCount += 1\n",
    "            if sCount == summSize:\n",
    "                return topSentences\n",
    "\n",
    "def lsi(sentTokens,numTopics):\n",
    "    \n",
    "    dct = corpora.Dictionary(sentTokens)\n",
    "    corpus = list(map(lambda st: dct.doc2bow(st), sentTokens))    \n",
    "    lsi = models.LsiModel(corpus, id2word=dct,num_topics=numTopics)\n",
    "    \n",
    "    vecCorpus = lsi[corpus]\n",
    "   \n",
    "    return vecCorpus            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Cumulative(lists):  \n",
    "    cu_list = []  \n",
    "    length = len(lists)  \n",
    "    cu_list = [sum(lists[0:x:1]) for x in range(0, length+1)]  \n",
    "    return cu_list[1:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectors_of_sentences(x,numTopics):\n",
    "    vectors = [None]*len(x)\n",
    "    for i,dv in enumerate(x):\n",
    "        array = [None]*numTopics\n",
    "        for sc in dv:\n",
    "            array[sc[0]] = sc[1]\n",
    "        vectors[i] = array\n",
    "        \n",
    "    return vectors  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_complaint_vectors(tokenized_sent,numTopics):\n",
    "    \n",
    "    df1['words_of_sents'] = tokenized_sent.apply(lambda y: tokenization(y))\n",
    "    complaint_corpus = []\n",
    "    \n",
    "    for i in range(len(df1['words_of_sents'])):\n",
    "        complaint_corpus += df1['words_of_sents'][i]\n",
    "\n",
    "    vectors_of_complaints = lsi(complaint_corpus,numTopics)\n",
    "\n",
    "    lengths_of_sentences = []\n",
    "    \n",
    "    for i in range(len(df1['words_of_sents'])):\n",
    "        lengths_of_sentences.append(len(df1['words_of_sents'][i]))\n",
    "    lengths_of_sentences.insert(0,0)\n",
    "\n",
    "    l = Cumulative(lengths_of_sentences)\n",
    "\n",
    "\n",
    "    for i in range(len(lengths_of_sentences)-1):\n",
    "        df1['vectors_of_corpus'][i] = vectors_of_complaints[l[i]:l[i+1]]\n",
    "            \n",
    "    df1['vectors_of_sents'] = df1['vectors_of_corpus'].apply(lambda x: vectors_of_sentences(x,numTopics))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lsi_summ(vecCorpus,numTopics,summary_length,sents):    \n",
    "\n",
    "    b=[]\n",
    "    \n",
    "    for i in range(len(df1['tokenized_sent'])):\n",
    "        if len(df1['tokenized_sent'][i])==1:\n",
    "            b.append(i)\n",
    "    \n",
    "    l = list(range(len(df1)))\n",
    "    \n",
    "    r = list(set(l)^set(b))\n",
    "    \n",
    "    for i in r:\n",
    "        \n",
    "        sortedVecs = list(map(lambda x: list(), range(numTopics)))\n",
    "\n",
    "        for j,dv in enumerate(vecCorpus[i]):\n",
    "            for sc in dv:\n",
    "                isc = (j, abs(sc[1]))\n",
    "                sortedVecs[sc[0]].append(isc)\n",
    "\n",
    "        sortedVecs = list(map(lambda iscl: sorted(iscl,key=takeSecond,reverse=True), sortedVecs))\n",
    "          \n",
    "    \n",
    "        top_sents = selTopSents(summary_length,numTopics,sortedVecs)       \n",
    "        top_sents = sorted(top_sents,key=takeFirst)\n",
    "        top_sentences = list(map(lambda ts: (ts[0],sents[i][ts[0]], ts[1]), top_sents))\n",
    "        \n",
    "        df1['summary'][i] = top_sentences\n",
    "        \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "summ_df = pd.DataFrame()\n",
    "summ_df.insert(0, \"summary\", [None]*len(df1), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summ_modification_index():\n",
    "    \n",
    "    x = []\n",
    "    modified = 0\n",
    "    for i in range(len(df1)):\n",
    "        temp1 = summ_df['summary'][i]\n",
    "        temp2 = df1['summary'][i]\n",
    "        x1 = []\n",
    "        x2 = []\n",
    "        for j in range(summary_length):\n",
    "            x1.append(temp1[j][1])\n",
    "            x2.append(temp2[j][1])\n",
    "        \n",
    "        x.append(list(set(x1)^set(x2)))\n",
    "        \n",
    "    for i in range(len(df1)):\n",
    "        if x[i]!=[]:\n",
    "            modified+=1\n",
    "    return round((modified/len(df1)),4)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class garbage_dictionary(dict): \n",
    "  \n",
    "    \n",
    "    def __init__(self): \n",
    "        self = dict() \n",
    "          \n",
    "    \n",
    "    def add(self, key, value): \n",
    "        self[key] = value \n",
    "        \n",
    "blacklist = garbage_dictionary()\n",
    "for i in range(len(df1)):\n",
    "        blacklist.add(i,[None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_blacklist(bl):\n",
    "    \n",
    "    f =  open('blacklist5_ub.txt')\n",
    "    black = f.readlines()    \n",
    "    f.close()\n",
    "    \n",
    "    i = input(\"pruning round # : \")\n",
    "    b = black[int(i)]\n",
    "    b = b.split(' ')\n",
    "    \n",
    "    for j in b:\n",
    "        if j=='\\n':\n",
    "            b.remove(j)\n",
    "\n",
    "    for j in b:\n",
    "        j = j.split(':')\n",
    "        key = int(j[0])\n",
    "        value = ast.literal_eval(j[1])\n",
    "        bl.add(key,value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_remove(comp,lis):\n",
    "    sent_to_remove = []\n",
    "    if lis!=[]:\n",
    "        for j in lis:\n",
    "            sent_to_remove.append(comp[j])\n",
    "        for k in sent_to_remove:\n",
    "            comp.remove(k)\n",
    "            \n",
    "def vect_remove(comp,lis):\n",
    "    vect_to_remove = []\n",
    "    if lis!=[]:\n",
    "        for j in lis:\n",
    "            vect_to_remove.append(comp[j])\n",
    "        for k in vect_to_remove:\n",
    "            comp.remove(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_sents(blacklist,vec,numTopics):\n",
    "    \n",
    "    b=[]\n",
    "    for k in blacklist.keys():\n",
    "        if blacklist[k]!=[None]:\n",
    "            b.append(k)\n",
    "    \n",
    "    l = list(range(len(df1)))\n",
    "    \n",
    "    r = list(set(l)^set(b))\n",
    "    \n",
    "    cosine_sim = list(map(lambda i: list(), range(len(df1))))\n",
    "    \n",
    "    \n",
    "    for i in r:\n",
    "        vectors = df1['vectors_of_sents'][i]        \n",
    "        for j in range(len(vectors)):\n",
    "            vec1 = vectors[j]\n",
    "            sim = dot(vec1,vec)/(norm(vec1)*norm(vec))           \n",
    "            cosine_sim[i].append([i,sim,j])\n",
    "    \n",
    "    x = []\n",
    "    for i in r:\n",
    "        x += comp_sim[i]\n",
    "    \n",
    "    top_sim = sorted(x,key=takeSecond,reverse=True)\n",
    "    x = None\n",
    "    top_sim = top_sim[0:50]\n",
    "    \n",
    "    #store all the sentences that the model prunes for future reference\n",
    "    for i in range(len(top_sim)):\n",
    "        comp_id = top_sim[i][0]\n",
    "        sent_id = top_sim[i][2]\n",
    "        pruned_data = open(\"pruned_data.txt\",mode=\"a\",encoding=\"utf-8\")\n",
    "        pruned_data.write(df1['tokenized_sent'][comp_id][sent_id]+ '\\n')\n",
    "        pruned_data.close()   \n",
    "    \n",
    "    \n",
    "    cos_sim = list(map(lambda i: list(), range(len(df1))))\n",
    "    \n",
    "    for i in range(len(top_sim)):\n",
    "        index = top_sim[i][0]\n",
    "        sentence_id = top_sim[i][2]\n",
    "        cos_sim[index].append(sentence_id)    \n",
    "            \n",
    "    \n",
    "    for i in range(len(cosine_sim)):\n",
    "        \n",
    "        sent_remove(df1['tokenized_sent'][i],cos_sim[i])\n",
    "        vect_remove(df1['vectors_of_sents'][i],cos_sim[i])      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_data(blacklist,numTopics):\n",
    "    \n",
    "    for i in range(len(blacklist)):\n",
    "        \n",
    "        if blacklist[i]!=[None]:\n",
    "            \n",
    "            sent_remove(df1['tokenized_sent'][i],blacklist[i]) \n",
    "            v = df1['vectors_of_sents'][i]\n",
    "            \n",
    "            for j in blacklist[i]:                                             \n",
    "                ref_v = v[j]\n",
    "                remove_sents(blacklist,ref_v,numTopics)\n",
    "                \n",
    "            vect_remove(df1['vectors_of_sents'][i],blacklist[i])\n",
    "                \n",
    "    \n",
    "    for i in range(len(df1)):\n",
    "        summ_df['summary'][i] = df1['summary'][i]\n",
    "    \n",
    "    blacklist.clear()\n",
    "    for i in range(len(df1)):\n",
    "        blacklist.add(i,[None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "modification_index = []\n",
    "relevance = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevance_score(keywords_n_grams):\n",
    "    \n",
    "   \n",
    "    \n",
    "    correct_pred = 0\n",
    "    for j in range(len(df1)):\n",
    "            x = df1['summary'][j]\n",
    "            issue = df1['Issue'][j]\n",
    "\n",
    "            match = []\n",
    "            key = None\n",
    "            summ =' '\n",
    "            for i in range(len(x)):\n",
    "                summ+= x[i][1]\n",
    "            summ = clean_text(summ)\n",
    "            summ = lemmatizer.lemmatize(summ)\n",
    "            summ1 = extract_ngrams(summ,1)\n",
    "            summ2 = extract_ngrams(summ,2)\n",
    "            summ = summ1+summ2\n",
    "            \n",
    "            for k in range(len(label)):\n",
    "                if label[k]==issue:\n",
    "                    key = keywords_list[k]\n",
    "                    \n",
    "            match = list(set(summ).intersection(key))\n",
    "            \n",
    "            if len(match)>1:\n",
    "                correct_pred+=1\n",
    "                \n",
    "\n",
    "    return round((correct_pred/len(df1)),4)   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_complaint_vectors(df1['tokenized_sent'],numTopics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsi_summ(df1['vectors_of_corpus'],numTopics,summary_length,df1['tokenized_sent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "modification_index.append(summ_modification_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevance.append(get_relevance_score(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['Issue'][]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['summary'][]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_blacklist = open(\"blacklist5_ub.txt\",mode=\"a\",encoding=\"utf-8\")\n",
    "add_blacklist.write('1240:[0] 1242:[4] 1244:[22] 1246:[10] 1248:[4] 1252:[9,14] 1257:[5,21] 1259:[1,17] 1263:[13] 1281:[3] 1282:[10] 1285:[4] 1286:[3,4] 1288:[7]\\n')\n",
    "add_blacklist.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_blacklist(blacklist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "prune_data(blacklist,numTopics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function for extracting keywords from complaints for all the labels\n",
    "Groups complaints belonging to a particular label and prepares a TFIDF table\n",
    "Then returns the top 'n' features where n is defined by the user\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# def assign_keywords(x,y,keywords_n_grams):\n",
    "    \n",
    "#     vectorizer = TfidfVectorizer(ngram_range =(keywords_n_grams,keywords_n_grams))\n",
    "#     vectors = vectorizer.fit_transform([x])\n",
    "#     feature_names = vectorizer.get_feature_names()\n",
    "#     dense = vectors.todense()\n",
    "#     denselist = dense.tolist()\n",
    "\n",
    "\n",
    "#     df2 = pd.DataFrame(denselist, columns=feature_names)\n",
    "#     df2 = df2.sort_values(by = 0 ,axis=1, ascending=False, inplace=False, kind='quicksort', na_position='last')\n",
    "\n",
    "#     features =[]\n",
    "#     for col in df2.columns:\n",
    "#         features.append(col)\n",
    "        \n",
    "#     return features[0:y]    \n",
    "\n",
    "\n",
    "\n",
    "# df1['cleaned_complaints'] = df1['Consumer complaint narrative'].apply(lambda x: clean_text(x))\n",
    "\n",
    "# labels= []\n",
    "# for i in df1['Issue'].unique():\n",
    "#     labels.append(i)\n",
    "    \n",
    "# issue_comp = list(map(lambda i: ' ', range(len(labels))))\n",
    "\n",
    "# for i in range(len(df1)):\n",
    "#     l = df1['Issue'][i]\n",
    "#     for j in range(len(labels)):\n",
    "#         lab = labels[j]\n",
    "#         if l==lab:\n",
    "#             issue_comp[j]+= ' ' + df1['cleaned_complaints'][i]     \n",
    "            \n",
    "\n",
    "# keywords = list(map(lambda i: ' ', range(len(labels))))\n",
    "\n",
    "# for i in range(len(labels)):\n",
    "#     keywords[i] = assign_keywords(lemmatizer.lemmatize(issue_comp[i]),30,1)        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
