{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_complaint_length = 8\n",
    "numTopics = 200\n",
    "summary_length = 2\n",
    "similarity_threshold = 0.8\n",
    "keywords_per_label = 100\n",
    "keywords_n_grams = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "import nltk\n",
    "from nltk import sent_tokenize,word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "\n",
    "import ast\n",
    "\n",
    "from gensim import corpora,models\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "  \n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions = { \n",
    "\"ain't\": \"am not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"needn't\": \"need not\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there had\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who's\": \"who is\",\n",
    "\"won't\": \"will not\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you're\": \"you are\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, remove_stopwords = True):    \n",
    "    \n",
    "    text = text.lower()\n",
    "    \n",
    "    \n",
    "    if True:\n",
    "        text = text.split()\n",
    "        new_text = []\n",
    "        for word in text:\n",
    "            if word in contractions:\n",
    "                new_text.append(contractions[word])\n",
    "            else:\n",
    "                new_text.append(word)\n",
    "        text = \" \".join(new_text)\n",
    "    \n",
    "    \n",
    "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\<a href', ' ', text)\n",
    "    text = re.sub(r'&amp;', '', text) \n",
    "    text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', text)\n",
    "    text = re.sub(r'<br />', ' ', text)\n",
    "    text = re.sub(r'\\'', ' ', text)\n",
    "    text = re.sub(r'xxxx', ' ', text)\n",
    "    text = re.sub(r'xx', ' ', text)\n",
    "    text = re.sub(r'xxx', ' ', text)\n",
    "    \n",
    "   \n",
    "    if remove_stopwords:\n",
    "        text = text.split()\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        text = [w for w in text if not w in stops]\n",
    "        text = \" \".join(text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('complaints.csv')\n",
    "df = df[['Issue','Consumer complaint narrative']]\n",
    "df = df[df['Consumer complaint narrative'].notna()]\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Issue</th>\n",
       "      <th>Consumer complaint narrative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Attempts to collect debt not owed</td>\n",
       "      <td>transworld systems inc. \\nis trying to collect...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Incorrect information on your report</td>\n",
       "      <td>I would like to request the suppression of the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Communication tactics</td>\n",
       "      <td>Over the past 2 weeks, I have been receiving e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fraud or scam</td>\n",
       "      <td>I was sold access to an event digitally, of wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Attempts to collect debt not owed</td>\n",
       "      <td>While checking my credit report I noticed thre...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Issue  \\\n",
       "0     Attempts to collect debt not owed   \n",
       "1  Incorrect information on your report   \n",
       "2                 Communication tactics   \n",
       "3                         Fraud or scam   \n",
       "4     Attempts to collect debt not owed   \n",
       "\n",
       "                        Consumer complaint narrative  \n",
       "0  transworld systems inc. \\nis trying to collect...  \n",
       "1  I would like to request the suppression of the...  \n",
       "2  Over the past 2 weeks, I have been receiving e...  \n",
       "3  I was sold access to an event digitally, of wh...  \n",
       "4  While checking my credit report I noticed thre...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Issue</th>\n",
       "      <th>Consumer complaint narrative</th>\n",
       "      <th>tokenized_sent</th>\n",
       "      <th>complaint_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Closing on a mortgage</td>\n",
       "      <td>We have already tried to contact the company w...</td>\n",
       "      <td>[We have already tried to contact the company ...</td>\n",
       "      <td>177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Incorrect information on your report</td>\n",
       "      <td>Accounts added and/or created unbeknownst to m...</td>\n",
       "      <td>[Accounts added and/or created unbeknownst to ...</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Trouble during payment process</td>\n",
       "      <td>I have been trying to get my Private Mortgage ...</td>\n",
       "      <td>[I have been trying to get my Private Mortgage...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Improper use of your report</td>\n",
       "      <td>2ND NOTICE OF PENDING LITIGATION SEEKING RELIE...</td>\n",
       "      <td>[2ND NOTICE OF PENDING LITIGATION SEEKING RELI...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Trouble during payment process</td>\n",
       "      <td>The mortgage company ( Roundpoint Mortgageg ) ...</td>\n",
       "      <td>[The mortgage company ( Roundpoint Mortgageg )...</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Issue  \\\n",
       "0                 Closing on a mortgage   \n",
       "1  Incorrect information on your report   \n",
       "2        Trouble during payment process   \n",
       "3           Improper use of your report   \n",
       "4        Trouble during payment process   \n",
       "\n",
       "                        Consumer complaint narrative  \\\n",
       "0  We have already tried to contact the company w...   \n",
       "1  Accounts added and/or created unbeknownst to m...   \n",
       "2  I have been trying to get my Private Mortgage ...   \n",
       "3  2ND NOTICE OF PENDING LITIGATION SEEKING RELIE...   \n",
       "4  The mortgage company ( Roundpoint Mortgageg ) ...   \n",
       "\n",
       "                                      tokenized_sent  complaint_length  \n",
       "0  [We have already tried to contact the company ...               177  \n",
       "1  [Accounts added and/or created unbeknownst to ...                30  \n",
       "2  [I have been trying to get my Private Mortgage...                14  \n",
       "3  [2ND NOTICE OF PENDING LITIGATION SEEKING RELI...                10  \n",
       "4  [The mortgage company ( Roundpoint Mortgageg )...                20  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tokenized_sent'] = df['Consumer complaint narrative'].apply(lambda x: sent_tokenize(x))\n",
    "df['complaint_length'] = df['tokenized_sent'].apply(lambda x: len(x))\n",
    "df = df[df['complaint_length']>=min_complaint_length]\n",
    "df = df.reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.head(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.drop(['complaint_length'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.insert(3, \"words_of_sents\", [None]*len(df1), True)\n",
    "df1.insert(4, \"vectors_of_corpus\", [None]*len(df1), True)\n",
    "df1.insert(5, \"vectors_of_sents\", [None]*len(df1), True)\n",
    "df1.insert(6, \"summary\", [None]*len(df1), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df1['tokenized_sent'])):\n",
    "    comp = df1['tokenized_sent'][i]\n",
    "    for j in comp:\n",
    "        v = word_tokenize(j)\n",
    "        if len(v)==1:\n",
    "            df1['tokenized_sent'][i].remove(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(sentences_list):\n",
    "    \n",
    "    words_list = [None]*len(sentences_list)\n",
    "    for i in range(len(sentences_list)):\n",
    "        words_list[i] = word_tokenize(sentences_list[i])\n",
    "    \n",
    "    return words_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def takeFirst(x):\n",
    "    return x[0]\n",
    "\n",
    "def takeSecond(x):\n",
    "    return x[1]\n",
    "\n",
    "def selTopSents(summSize, numTopics, sortedVecs):\n",
    "    topSentences = []\n",
    "    sentIndexes = set()\n",
    "    sCount = 0\n",
    "    \n",
    "    for i in range(summSize):\n",
    "        for j in range(numTopics):\n",
    "            vecs = sortedVecs[j]                       \n",
    "            si = vecs[i][0]\n",
    "            \n",
    "            if si not in sentIndexes:\n",
    "                topSentences.append(vecs[i])\n",
    "                sentIndexes.add(si)\n",
    "                sCount += 1\n",
    "            if sCount == summSize:\n",
    "                return topSentences\n",
    "\n",
    "def lsi(sentTokens,numTopics):\n",
    "    \n",
    "    dct = corpora.Dictionary(sentTokens)\n",
    "    corpus = list(map(lambda st: dct.doc2bow(st), sentTokens))    \n",
    "    lsi = models.LsiModel(corpus, id2word=dct,num_topics=numTopics)\n",
    "    \n",
    "    vecCorpus = lsi[corpus]\n",
    "   \n",
    "    return vecCorpus            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Cumulative(lists):  \n",
    "    cu_list = []  \n",
    "    length = len(lists)  \n",
    "    cu_list = [sum(lists[0:x:1]) for x in range(0, length+1)]  \n",
    "    return cu_list[1:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectors_of_sentences(x,numTopics):\n",
    "    vectors = [None]*len(x)\n",
    "    for i,dv in enumerate(x):\n",
    "        array = [None]*numTopics\n",
    "        for sc in dv:\n",
    "            array[sc[0]] = sc[1]\n",
    "        vectors[i] = array\n",
    "        \n",
    "    return vectors  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_complaint_vectors(tokenized_sent,numTopics):\n",
    "    \n",
    "    df1['words_of_sents'] = tokenized_sent.apply(lambda y: tokenization(y))\n",
    "    complaint_corpus = []\n",
    "    \n",
    "    for i in range(len(df1['words_of_sents'])):\n",
    "        complaint_corpus += df1['words_of_sents'][i]\n",
    "\n",
    "    vectors_of_complaints = lsi(complaint_corpus,numTopics)\n",
    "\n",
    "    lengths_of_sentences = []\n",
    "    \n",
    "    for i in range(len(df1['words_of_sents'])):\n",
    "        lengths_of_sentences.append(len(df1['words_of_sents'][i]))\n",
    "    lengths_of_sentences.insert(0,0)\n",
    "\n",
    "    l = Cumulative(lengths_of_sentences)\n",
    "\n",
    "\n",
    "    for i in range(len(lengths_of_sentences)-1):\n",
    "        df1['vectors_of_corpus'][i] = vectors_of_complaints[l[i]:l[i+1]]\n",
    "            \n",
    "    df1['vectors_of_sents'] = df1['vectors_of_corpus'].apply(lambda x: vectors_of_sentences(x,numTopics))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lsi_summ(vecCorpus,numTopics,summary_length,sents):    \n",
    "\n",
    "    b=[]\n",
    "    \n",
    "    for i in range(len(df1['tokenized_sent'])):\n",
    "        if len(df1['tokenized_sent'][i])==1:\n",
    "            b.append(i)\n",
    "    \n",
    "    l = list(range(len(df1)))\n",
    "    \n",
    "    r = list(set(l)^set(b))\n",
    "    \n",
    "    for i in r:\n",
    "        \n",
    "        sortedVecs = list(map(lambda x: list(), range(numTopics)))\n",
    "\n",
    "        for j,dv in enumerate(vecCorpus[i]):\n",
    "            for sc in dv:\n",
    "                isc = (j, abs(sc[1]))\n",
    "                sortedVecs[sc[0]].append(isc)\n",
    "\n",
    "        sortedVecs = list(map(lambda iscl: sorted(iscl,key=takeSecond,reverse=True), sortedVecs))\n",
    "          \n",
    "    \n",
    "        top_sents = selTopSents(summary_length,numTopics,sortedVecs)       \n",
    "        top_sents = sorted(top_sents,key=takeFirst)\n",
    "        top_sentences = list(map(lambda ts: (ts[0],sents[i][ts[0]], ts[1]), top_sents))\n",
    "        \n",
    "        df1['summary'][i] = top_sentences\n",
    "        \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "summ_df = pd.DataFrame()\n",
    "summ_df.insert(0, \"summary\", [None]*5000, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summ_modification_index():\n",
    "    \n",
    "    x = []\n",
    "    modified = 0\n",
    "    for i in range(len(df1)):\n",
    "        temp1 = summ_df['summary'][i]\n",
    "        temp2 = df1['summary'][i]\n",
    "        x1 = []\n",
    "        x2 = []\n",
    "        for j in range(summary_length):\n",
    "            x1.append(temp1[j][1])\n",
    "            x2.append(temp2[j][1])\n",
    "        \n",
    "        x.append(list(set(x1)^set(x2)))\n",
    "        \n",
    "    for i in range(len(df1)):\n",
    "        if x[i]!=[]:\n",
    "            modified+=1\n",
    "    return modified/len(df1)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class garbage_dictionary(dict): \n",
    "  \n",
    "    \n",
    "    def __init__(self): \n",
    "        self = dict() \n",
    "          \n",
    "    \n",
    "    def add(self, key, value): \n",
    "        self[key] = value \n",
    "        \n",
    "blacklist = garbage_dictionary()\n",
    "for i in range(len(df1)):\n",
    "        blacklist.add(i,[None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_blacklist(bl):\n",
    "    \n",
    "    f =  open('blacklist_0.8.txt')\n",
    "    black = f.readlines()    \n",
    "    f.close()\n",
    "    \n",
    "    i = input(\"pruning round # : \")\n",
    "    b = black[int(i)]\n",
    "    b = b.split(' ')\n",
    "    \n",
    "    for j in b:\n",
    "        if j=='\\n':\n",
    "            b.remove(j)\n",
    "\n",
    "    for j in b:\n",
    "        j = j.split(':')\n",
    "        key = int(j[0])\n",
    "        value = ast.literal_eval(j[1])\n",
    "        bl.add(key,value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_remove(comp,lis):\n",
    "    sent_to_remove = []\n",
    "    if lis!=[]:\n",
    "        for j in lis:\n",
    "            sent_to_remove.append(comp[j])\n",
    "        for k in sent_to_remove:\n",
    "            comp.remove(k)\n",
    "            \n",
    "def vect_remove(comp,lis):\n",
    "    vect_to_remove = []\n",
    "    if lis!=[]:\n",
    "        for j in lis:\n",
    "            vect_to_remove.append(comp[j])\n",
    "        for k in vect_to_remove:\n",
    "            comp.remove(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_sents(blacklist,vec,numTopics,similarity_threshold):\n",
    "    \n",
    "    b=[]\n",
    "    for k in blacklist.keys():\n",
    "        if blacklist[k]!=[None]:\n",
    "            b.append(k)\n",
    "    \n",
    "    l = list(range(len(df1)))\n",
    "    \n",
    "    r = list(set(l)^set(b))\n",
    "    \n",
    "    cosine_sim = list(map(lambda i: list(), range(len(df1))))\n",
    "    \n",
    "    for i in r:\n",
    "        \n",
    "        vectors = df1['vectors_of_sents'][i]        \n",
    "        for j in range(len(vectors)):\n",
    "            vec1 = vectors[j]\n",
    "            sim = dot(vec1,vec)/(norm(vec1)*norm(vec))\n",
    "            if sim>=similarity_threshold:\n",
    "                cosine_sim[i].append(j)\n",
    "            \n",
    "    \n",
    "    for i in range(len(cosine_sim)):\n",
    "        \n",
    "        sent_remove(df1['tokenized_sent'][i],cosine_sim[i])\n",
    "        vect_remove(df1['vectors_of_sents'][i],cosine_sim[i])      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_data(blacklist,numTopics,similarity_threshold):\n",
    "    \n",
    "    for i in range(len(blacklist)):\n",
    "        \n",
    "        if blacklist[i]!=[None]:\n",
    "            \n",
    "            sent_remove(df1['tokenized_sent'][i],blacklist[i]) \n",
    "            v = df1['vectors_of_sents'][i]\n",
    "            \n",
    "            for j in blacklist[i]:                                             \n",
    "                ref_v = v[j]\n",
    "                remove_sents(blacklist,ref_v,numTopics,similarity_threshold)\n",
    "                \n",
    "            vect_remove(df1['vectors_of_sents'][i],blacklist[i])\n",
    "                \n",
    "    \n",
    "    for i in range(len(df1)):\n",
    "        summ_df['summary'][i] = df1['summary'][i]\n",
    "    \n",
    "    blacklist.clear()\n",
    "    for i in range(len(df1)):\n",
    "        blacklist.add(i,[None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_keywords(x,y,keywords_n_grams):\n",
    "    vectorizer = TfidfVectorizer(ngram_range =(keywords_n_grams,keywords_n_grams))\n",
    "    vectors = vectorizer.fit_transform([x])\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    dense = vectors.todense()\n",
    "    denselist = dense.tolist()\n",
    "\n",
    "\n",
    "    df2 = pd.DataFrame(denselist, columns=feature_names)\n",
    "    df2 = df2.sort_values(by = 0 ,axis=1, ascending=False, inplace=False, kind='quicksort', na_position='last')\n",
    "\n",
    "    features =[]\n",
    "    for col in df2.columns:\n",
    "        features.append(col)\n",
    "        \n",
    "    return features[0:y]    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['cleaned_complaints'] = df1['Consumer complaint narrative'].apply(lambda x: clean_text(x))\n",
    "\n",
    "labels= []\n",
    "for i in df1['Issue'].unique():\n",
    "    labels.append(i)\n",
    "    \n",
    "issue_comp = list(map(lambda i: ' ', range(len(labels))))\n",
    "\n",
    "for i in range(len(df1)):\n",
    "    l = df1['Issue'][i]\n",
    "    for j in range(len(labels)):\n",
    "        lab = labels[j]\n",
    "        if l==lab:\n",
    "            issue_comp[j]+= ' ' + df1['cleaned_complaints'][i]           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ngrams(data, num):\n",
    "    n_grams = TextBlob(data).ngrams(num)\n",
    "    return [ ' '.join(grams) for grams in n_grams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = list(map(lambda i: ' ', range(len(labels))))\n",
    "\n",
    "for i in range(len(labels)):\n",
    "    keywords[i] = assign_keywords(lemmatizer.lemmatize(issue_comp[i]),keywords_per_label,keywords_n_grams)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "modification_index = []\n",
    "relevance = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevance_score(keywords_n_grams):\n",
    "    \n",
    "    correct_pred = 0\n",
    "    for j in range(len(df1)):\n",
    "            x = df1['summary'][j]\n",
    "            issue = df1['Issue'][j]\n",
    "\n",
    "            matches = [None]*len(keywords)\n",
    "            summ =' '\n",
    "            for i in range(len(x)):\n",
    "                summ+= x[i][1]\n",
    "            summ = clean_text(summ)\n",
    "            summ = lemmatizer.lemmatize(summ)\n",
    "            summ = extract_ngrams(summ,keywords_n_grams)\n",
    "\n",
    "            for i in range(len(keywords)):\n",
    "                matches[i] = list(set(summ).intersection(keywords[i]))\n",
    "\n",
    "            no_matches = []\n",
    "\n",
    "            for i in range(len(matches)):\n",
    "                no_matches.append(len(matches[i]))\n",
    "               \n",
    "            m = max(no_matches)\n",
    "            \n",
    "            if m!=0:\n",
    "                match_list = []\n",
    "                for k in range(len(no_matches)):\n",
    "                    if no_matches[k] == m:\n",
    "                        match_list.append(labels[k])\n",
    "\n",
    "                for k in match_list:\n",
    "                    if k==issue:\n",
    "                        correct_pred+=1\n",
    "                        break          \n",
    "    \n",
    "    \n",
    "    return correct_pred/len(df1)   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_complaint_vectors(df1['tokenized_sent'],numTopics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsi_summ(df1['vectors_of_corpus'],numTopics,summary_length,df1['tokenized_sent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevance.append(get_relevance_score(keywords_n_grams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.2712, 0.2598, 0.2614, 0.2604, 0.2666, 0.2666, 0.2664, 0.266, 0.267, 0.0]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pruning round # : 8\n"
     ]
    }
   ],
   "source": [
    "update_blacklist(blacklist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'NoneType' and 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-84-f9d5f05f9c9b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprune_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblacklist\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnumTopics\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msimilarity_threshold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-24-c96dffc1e1ec>\u001b[0m in \u001b[0;36mprune_data\u001b[1;34m(blacklist, numTopics, similarity_threshold)\u001b[0m\n\u001b[0;32m     10\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mblacklist\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m                 \u001b[0mref_v\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m                 \u001b[0mremove_sents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblacklist\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mref_v\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnumTopics\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msimilarity_threshold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[0mvect_remove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'vectors_of_sents'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mblacklist\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-23-47e3a26b0d75>\u001b[0m in \u001b[0;36mremove_sents\u001b[1;34m(blacklist, vec, numTopics, similarity_threshold)\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvectors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m             \u001b[0mvec1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectors\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m             \u001b[0msim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvec1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvec1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0msim\u001b[0m\u001b[1;33m>=\u001b[0m\u001b[0msimilarity_threshold\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m                 \u001b[0mcosine_sim\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'NoneType' and 'float'"
     ]
    }
   ],
   "source": [
    "prune_data(blacklist,numTopics,similarity_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_blacklist = open(\"blacklist.txt\",mode=\"a\",encoding=\"utf-8\")\n",
    "add_blacklist.write('3000:[12] 3003:[6] 3006:[2] 3010:[14] 3012:[9] 3021:[9] 3022:[4] 3023:[3] 3024:[0] 3026:[1] 3028:[6] 3033:[3] 3040:[3] 3042:[7,11] 3044:[11] 3045:[6] 3046:[6,7] 3047:[13] 3049:[7] 3050:[4]\\n')\n",
    "add_blacklist.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "modification_index.append(summ_modification_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.693, 0.0104, 0.137, 0.0404, 0.03, 0.0288, 0.1006, 0.0104, 0.01, 0.2296]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modification_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-c08934982a70>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mcomp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'words_of_sents'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mcomp\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mdf1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'words_of_sents'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df1' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
